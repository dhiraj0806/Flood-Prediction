# -*- coding: utf-8 -*-
"""Flood Prediction(PBL).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O23DhehuieYS-5Vb1hbKZOHhRjAbO6bV
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

ds = pd.read_csv('kerala.csv')

print(ds)

ds = ds.drop(['SUBDIVISION','YEAR'] , axis=1)

print(ds)

ds.shape

print(ds.FLOODS.value_counts())

ds.corr()

ds.apply(lambda x:sum(x.isnull()), axis = 0)

ds['FLOODS'].replace(['YES', 'NO'], [1, 0], inplace = True)

X = ds.iloc[:,:-1]         #independent variable
print(X)

y = ds.iloc[:,-1]         #dependent variable
print(y)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline                           
c = ds[['JUN', 'JUL', 'AUG', 'SEP']]         
c.hist()
plt.show()

ax = ds[['JAN', 'FEB', 'MAR', 'APR','MAY', 'JUN', 'AUG', 'SEP', 'OCT','NOV','DEC']].mean().plot.bar(width=0.5,edgecolor='k',align='center',linewidth=2,figsize=(14,6))
plt.xlabel('Month',fontsize=30)
plt.ylabel('Monthly Rainfall',fontsize=20)
plt.title('Rainfall in Kerela for all Months',fontsize=25)
ax.tick_params(labelsize=20)
plt.grid()
plt.ioff()



"""Due to the wide distribution of the dataset we scale the data in the range 0 to 1"""

from sklearn import preprocessing     
minmax = preprocessing.MinMaxScaler(feature_range=(0,1))
minmax.fit_transform(X)

from sklearn import model_selection
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

X_train.head()

X_test.head()

y_train.head()

y_test.head()

"""**1. KNN CLASSIFIER**"""

from sklearn import neighbors

clf = neighbors.KNeighborsClassifier()
knn_clf = clf.fit(X_train,y_train)

y_predict = knn_clf.predict(X_test)
print('predicted chances of flood')
print(y_predict)

print("actual values of floods:")
print(y_test)

from sklearn.model_selection import cross_val_score
knn_accuracy = cross_val_score(knn_clf,X_test,y_test,cv=3,scoring='accuracy',n_jobs=-1)
knn_accuracy.mean()

"""**2. Logistic Regression**"""

X_train_std = minmax.fit_transform(X_train)
X_test_std = minmax.transform(X_test)

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr_clf = lr.fit(X_train_std,y_train)

lr_accuracy = cross_val_score(lr_clf,X_test_std,y_test,cv=3,scoring='accuracy',n_jobs=-1)

lr_accuracy.mean()

y_predict = lr_clf.predict(X_test_std)
print('Predicted chances of flood')
print(y_predict)

print('Actual chances of flood')
print(y_test.values)

from sklearn.metrics import accuracy_score,recall_score,roc_auc_score,confusion_matrix
print("\naccuracy score: %f"%(accuracy_score(y_test,y_predict)*100))
print("recall score: %f"%(recall_score(y_test,y_predict)*100))
print("roc score: %f"%(roc_auc_score(y_test,y_predict)*100))

"""**3. DECISION TREE**"""

from sklearn.tree import DecisionTreeClassifier
dtc_clf = DecisionTreeClassifier()
dtc_clf.fit(X_train,y_train)
dtc_clf_acc = cross_val_score(dtc_clf,X_train_std,y_train,cv=3,scoring="accuracy",n_jobs=-1)
dtc_clf_acc

y_pred = dtc_clf.predict(X_test)
print(y_pred)

print("actual values:")
print(y_test.values)

from sklearn.metrics import accuracy_score,recall_score,roc_auc_score,confusion_matrix
print("\naccuracy score:%f"%(accuracy_score(y_test,y_pred)*100))
print("recall score:%f"%(recall_score(y_test,y_pred)*100))
print("roc score:%f"%(roc_auc_score(y_test,y_pred)*100))

"""**4. Random Forest Classification**



"""

from sklearn.ensemble import RandomForestClassifier
rmf = RandomForestClassifier(max_depth=3, random_state=0)
rmf_clf = rmf.fit(X_train,y_train)
rmf_clf

rmf_clf_acc = cross_val_score(rmf_clf,X_train_std,y_train,cv=3,scoring="accuracy",n_jobs=-1)

rmf_clf_acc

y_pred = rmf_clf.predict(X_test)

print(y_pred)

from sklearn.metrics import accuracy_score,recall_score,roc_auc_score,confusion_matrix
print("\naccuracy score:%f"%(accuracy_score(y_test,y_pred)*100))
print("recall score:%f"%(recall_score(y_test,y_pred)*100))
print("roc score:%f"%(roc_auc_score(y_test,y_pred)*100))

"""**5. Enseble Learning**"""

from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

log_clf = LogisticRegression(solver="liblinear", random_state=42)
rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)
knn_clf = KNeighborsClassifier()

voting = VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('knn', knn_clf)],
    voting='hard')

voting_clf = voting.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

for clf in (log_clf, rnd_clf, knn_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))

models = []
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
models.append(('KNN', KNeighborsClassifier()))
models.append(('LR', LogisticRegression(solver = 'liblinear', random_state=42)))
models.append(('DT', DecisionTreeClassifier()))
models.append(('RF', RandomForestClassifier(n_estimators = 10,random_state=42)))
models.append(('EL', VotingClassifier(
    estimators=[('lr', log_clf), ('rf', rnd_clf), ('knn', knn_clf)],
    voting='hard')))


names = []
scores = []
for name, model in models:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    scores.append(accuracy_score(y_test, y_pred))
    names.append(name)
tr_split = pd.DataFrame({'Name': names, 'Score': scores})
print(tr_split)

import seaborn as sns
axis = sns.barplot(x = 'Name', y = 'Score', data =tr_split )
axis.set(xlabel='Classifier', ylabel='Accuracy')
for p in axis.patches:
    height = p.get_height()
    axis.text(p.get_x() + p.get_width()/2, height + 0.005, '{:1.4f}'.format(height), ha="center") 
    
plt.show()